base_config: base_config.yaml

model:
  num_layers: 8          
  future_horizon: 1
  d_embed: 512
  pretrained_input_dim: 512
  latent_summary_enabled: false
  # latent_summary_num_latents: 8
  use_output_projector: true

data:
  future_horizon: 1
  num_full_res_frames: 10
  # history_prefix_frames: 10

training:
  objective: residue_centric

  res_ce_weight: 2.0
  res_js_weight: 0.0
  alignment_loss_weight: 0.0
  alignment_checkpoint_path: "output/checkpoints/aligned_lookup.pt"
  freeze_alignment_weights: false
  embedding_lr_scale: 0.1
  alignment_warmup_epochs: 0
  # change_upweight: 10.0

  # optimization
  learning_rate: 1.0e-4
  label_smoothing: 0.02
  
  # Slowly introduce model's own predictions to bridge train/eval gap
  scheduled_sampling_p: 0.5

  # scheduler (overrides base_config defaults)
  use_scheduler: true
  # Full dataset: ~150,000 samples / 4 batch = 37,500 batches. 37,500 / 32 accum = ~1172 steps/epoch
  estimated_steps_per_epoch: 1172
  # Standard warmup
  warmup_steps: 1000
  max_epochs: 5      

  grad_accum_steps: 32
  # No train_only_proteins -> uses full dataset
  # Warmup alignment for 1 epoch
  
experiment:
  run_name: "full_LLDR0.1_CE2_outputproj_f1"
  tags: ["residue_centric", "full_dataset", "alignment", "CE2"]
  