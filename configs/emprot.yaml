base_config: base_config.yaml

model:
  d_embed: 128
  num_heads: 4
  num_layers: 2
  dropout: 0.1
  latent_summary_enabled: true
  latent_summary_num_latents: 32
  latent_summary_heads: 4
  latent_summary_dropout: 0.1
  latent_summary_max_prefix: 200   # L: max older frames summarized per horizon

data:
  data_dir: "/scratch/groups/rbaltman/ziyiw23/traj_embeddings"
  metadata_path: "/oak/stanford/groups/rbaltman/ziyiw23/EMPROT/traj_metadata.csv"
  batch_size: 4
  history_prefix_frames: 5      # L: summarized older history frames
  num_full_res_frames: 5     # history frames (K)
  stride: 1
  future_horizon: 10          # prediction steps (F)
  num_workers: 4
  seed: 42

training:
  learning_rate: 1.0e-3
  weight_decay: 0.01
  max_epochs: 10
  patience: 5
  use_scheduler: false       # cosine scheduler optional (uses transformers if available)
  warmup_proportion: 0.1
  use_amp: true
  max_grad_norm: 0.5
  grad_accum_steps: 1
  log_interval: 200

experiment:
  run_name: "emprot_classify_l5_k5_f10"
  use_wandb: true
  wandb_project: "emprot"
  entity: null
  tags: ["emprot", "classification"]
  notes: "Classification-only training with minimal config"
