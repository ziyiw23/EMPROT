base_config: base_config.yaml

model:
  num_layers: 8          
  future_horizon: 3
  d_embed: 512
  pretrained_input_dim: 512
  use_output_projector: true

data:
  future_horizon: 1
  num_full_res_frames: 5
  history_prefix_frames: 0

training:
  objective: residue_centric

  res_ce_weight: 2.0
  res_js_weight: 0.0
  alignment_loss_weight: 0.0001
  freeze_alignment_weights: false
  embedding_lr_scale: 0.1
  # optimization
  learning_rate: 1.0e-4
  label_smoothing: 0.02
  # Slowly introduce model's own predictions to bridge train/eval gap
  scheduled_sampling_p: 0.5

  # scheduler (overrides base_config defaults)
  use_scheduler: true
  # Single protein: ~2000 frames / 4 batch = 500 batches. 500 / 32 accum = ~16 steps/epoch
  estimated_steps_per_epoch: 16
  # Short warmup for small dataset
  warmup_steps: 50
  # More epochs to get enough steps (16 * 100 = 1600 steps)
  max_epochs: 100          

  grad_accum_steps: 32
  train_only_proteins:
    - "17822_dyn_802_traj_15785"
  max_train_proteins: 1
  # Warmup alignment for 10 epochs (since epochs are short)
  alignment_checkpoint_path: "output/checkpoints/aligned_lookup.pt"
  freeze_alignment_weights: true
  alignment_warmup_epochs: 0

experiment:
  run_name: "1prot_f1_lr0.1_alljs_outputproj"
  tags: ["residue_centric", "overfit", "debug", "alignment"]
